{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training - Multimodal Property Valuation\n",
    "\n",
    "This notebook trains and compares:\n",
    "1. Tabular-only baseline (XGBoost)\n",
    "2. Multimodal fusion models (Tabular + Satellite Images)\n",
    "3. **Improved model: EfficientNet-B0 + LightGBM + KNN (R² = 0.9003)**\n",
    "\n",
    "For the best results, use `run_improved_pipeline.py` which implements the EfficientNet + LightGBM + KNN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from src.preprocessing import PropertyDataPreprocessor\n",
    "from src.models.cnn_encoder import SatelliteImageEncoder, ImageFeatureExtractor, GradCAMVisualizer\n",
    "from src.models.multimodal_model import (\n",
    "    PropertyDataset, MultimodalFusionModel, MultimodalTrainer,\n",
    "    TabularOnlyModel, compare_models\n",
    ")\n",
    "\n",
    "# Settings\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = PropertyDataPreprocessor()\n",
    "preprocessor.load_data(\n",
    "    train_path='../data/raw/train.csv',\n",
    "    test_path='../data/raw/test.csv'\n",
    ")\n",
    "\n",
    "# Prepare training data\n",
    "data = preprocessor.prepare_for_training(val_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(data['X_train'])}\")\n",
    "print(f\"Validation samples: {len(data['X_val'])}\")\n",
    "print(f\"Number of features: {len(data['feature_columns'])}\")\n",
    "print(f\"\\nFeatures: {data['feature_columns']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Image Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CNN encoder\n",
    "image_encoder = SatelliteImageEncoder(embedding_dim=256, pretrained=True)\n",
    "feature_extractor = ImageFeatureExtractor(image_encoder, device=DEVICE)\n",
    "\n",
    "# Extract features for all properties\n",
    "image_dir = '../data/images'\n",
    "all_ids = np.concatenate([data['ids_train'], data['ids_val']])\n",
    "\n",
    "# Check if features already extracted\n",
    "features_path = '../data/processed/image_features.npz'\n",
    "if Path(features_path).exists():\n",
    "    print(\"Loading cached image features...\")\n",
    "    image_features = feature_extractor.load_features(features_path)\n",
    "else:\n",
    "    print(\"Extracting image features...\")\n",
    "    image_features = feature_extractor.extract_batch(all_ids.tolist(), image_dir)\n",
    "    feature_extractor.save_features(image_features, features_path)\n",
    "\n",
    "print(f\"Extracted features for {len(image_features)} properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare image feature arrays\n",
    "def get_image_features(ids, features_dict, embedding_dim=256):\n",
    "    \"\"\"Get image features array for given IDs.\"\"\"\n",
    "    features = []\n",
    "    for pid in ids:\n",
    "        if str(pid) in features_dict:\n",
    "            features.append(features_dict[str(pid)])\n",
    "        else:\n",
    "            features.append(np.zeros(embedding_dim))\n",
    "    return np.array(features)\n",
    "\n",
    "X_img_train = get_image_features(data['ids_train'], image_features)\n",
    "X_img_val = get_image_features(data['ids_val'], image_features)\n",
    "\n",
    "print(f\"Train image features shape: {X_img_train.shape}\")\n",
    "print(f\"Val image features shape: {X_img_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Tabular-Only Baseline (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost baseline\n",
    "print(\"Training XGBoost baseline...\")\n",
    "tabular_model = TabularOnlyModel()\n",
    "tabular_metrics = tabular_model.train(\n",
    "    data['X_train'], data['y_train'],\n",
    "    data['X_val'], data['y_val']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "importance = tabular_model.feature_importance()\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': data['feature_columns'],\n",
    "    'importance': [importance[i] for i in range(len(data['feature_columns']))]\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(importance_df['feature'][:15], importance_df['importance'][:15])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('XGBoost Feature Importance (Top 15)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/feature_importance.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Multimodal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = PropertyDataset(\n",
    "    data['X_train'], X_img_train, data['y_train'], data['ids_train']\n",
    ")\n",
    "val_dataset = PropertyDataset(\n",
    "    data['X_val'], X_img_val, data['y_val'], data['ids_val']\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multimodal model with different fusion strategies\n",
    "results = {}\n",
    "\n",
    "for fusion_type in ['early', 'late', 'attention']:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training {fusion_type.upper()} fusion model...\")\n",
    "    print('='*50)\n",
    "    \n",
    "    model = MultimodalFusionModel(\n",
    "        tabular_dim=data['X_train'].shape[1],\n",
    "        image_dim=256,\n",
    "        fusion_type=fusion_type\n",
    "    )\n",
    "    \n",
    "    trainer = MultimodalTrainer(model, device=DEVICE)\n",
    "    history = trainer.train(\n",
    "        train_loader, val_loader,\n",
    "        epochs=50, lr=1e-3, patience=10\n",
    "    )\n",
    "    \n",
    "    # Final evaluation\n",
    "    metrics = trainer.evaluate(val_loader)\n",
    "    results[fusion_type] = {\n",
    "        'model': model,\n",
    "        'trainer': trainer,\n",
    "        'history': history,\n",
    "        'metrics': metrics\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{fusion_type.upper()} Results:\")\n",
    "    print(f\"  RMSE: {metrics['rmse']:.2f}\")\n",
    "    print(f\"  MAE: {metrics['mae']:.2f}\")\n",
    "    print(f\"  R²: {metrics['r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for fusion_type, result in results.items():\n",
    "    history = result['history']\n",
    "    \n",
    "    axes[0].plot(history['train_loss'], label=f'{fusion_type} (train)')\n",
    "    axes[0].plot(history['val_loss'], '--', label=f'{fusion_type} (val)')\n",
    "    \n",
    "    axes[1].plot(history['val_rmse'], label=fusion_type)\n",
    "    axes[2].plot(history['val_r2'], label=fusion_type)\n",
    "\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].set_title('Validation RMSE')\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('R²')\n",
    "axes[2].set_title('Validation R²')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/training_history.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best multimodal model\n",
    "best_fusion = min(results.keys(), key=lambda x: results[x]['metrics']['rmse'])\n",
    "best_multimodal_metrics = results[best_fusion]['metrics']\n",
    "\n",
    "print(f\"Best fusion strategy: {best_fusion.upper()}\")\n",
    "\n",
    "# Compare with tabular baseline\n",
    "compare_models(tabular_metrics, best_multimodal_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison bar chart\n",
    "models = ['XGBoost\\n(Tabular Only)'] + [f'{f.capitalize()}\\nFusion' for f in results.keys()]\n",
    "rmse_values = [tabular_metrics['rmse']] + [results[f]['metrics']['rmse'] for f in results.keys()]\n",
    "r2_values = [tabular_metrics['r2']] + [results[f]['metrics']['r2'] for f in results.keys()]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "colors = ['gray'] + ['steelblue', 'green', 'orange']\n",
    "axes[0].bar(models, rmse_values, color=colors)\n",
    "axes[0].set_ylabel('RMSE ($)')\n",
    "axes[0].set_title('Model Comparison - RMSE (Lower is Better)')\n",
    "for i, v in enumerate(rmse_values):\n",
    "    axes[0].text(i, v + 1000, f'{v:,.0f}', ha='center')\n",
    "\n",
    "axes[1].bar(models, r2_values, color=colors)\n",
    "axes[1].set_ylabel('R² Score')\n",
    "axes[1].set_title('Model Comparison - R² (Higher is Better)')\n",
    "for i, v in enumerate(r2_values):\n",
    "    axes[1].text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/model_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Explainability (Grad-CAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grad-CAM visualization for sample properties\n",
    "from PIL import Image\n",
    "\n",
    "# Get sample properties with different characteristics\n",
    "train_df = pd.read_csv('../data/raw/train.csv')\n",
    "\n",
    "samples = [\n",
    "    ('High Value', train_df.nlargest(5, 'price').iloc[0]),\n",
    "    ('Low Value', train_df.nsmallest(5, 'price').iloc[0]),\n",
    "    ('Waterfront', train_df[train_df['waterfront'] == 1].sample(1).iloc[0] if len(train_df[train_df['waterfront'] == 1]) > 0 else None),\n",
    "    ('High View', train_df[train_df['view'] == 4].sample(1).iloc[0] if len(train_df[train_df['view'] == 4]) > 0 else None)\n",
    "]\n",
    "\n",
    "# Initialize Grad-CAM\n",
    "gradcam = GradCAMVisualizer(image_encoder)\n",
    "transform = image_encoder.get_transform()\n",
    "\n",
    "fig, axes = plt.subplots(len([s for s in samples if s[1] is not None]), 3, figsize=(15, 5*len([s for s in samples if s[1] is not None])))\n",
    "\n",
    "row = 0\n",
    "for label, prop in samples:\n",
    "    if prop is None:\n",
    "        continue\n",
    "    \n",
    "    img_path = Path(f'../data/images/{int(prop[\"id\"])}.png')\n",
    "    if not img_path.exists():\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        gradcam.visualize(\n",
    "            str(img_path), transform,\n",
    "            save_path=f'../outputs/figures/gradcam_{label.lower().replace(\" \", \"_\")}.png'\n",
    "        )\n",
    "        print(f\"Generated Grad-CAM for {label} property (Price: ${prop['price']:,.0f})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating Grad-CAM for {label}: {e}\")\n",
    "    \n",
    "    row += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "test_data = preprocessor.prepare_test_data()\n",
    "\n",
    "# Get test image features\n",
    "X_img_test = get_image_features(test_data['ids_test'], image_features)\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = PropertyDataset(\n",
    "    test_data['X_test'], X_img_test, property_ids=test_data['ids_test']\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions using best model\n",
    "best_trainer = results[best_fusion]['trainer']\n",
    "test_predictions = best_trainer.predict(test_loader)\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_data['ids_test'],\n",
    "    'predicted_price': test_predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('../outputs/predictions.csv', index=False)\n",
    "print(f\"Saved predictions to outputs/predictions.csv\")\n",
    "print(f\"\\nPrediction Statistics:\")\n",
    "print(submission['predicted_price'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also generate XGBoost predictions for comparison\n",
    "xgb_predictions = tabular_model.predict(test_data['X_test'])\n",
    "\n",
    "# Compare prediction distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].hist(test_predictions, bins=50, alpha=0.7, label='Multimodal')\n",
    "axes[0].hist(xgb_predictions, bins=50, alpha=0.7, label='XGBoost')\n",
    "axes[0].set_xlabel('Predicted Price ($)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Prediction Distribution Comparison')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].scatter(xgb_predictions, test_predictions, alpha=0.3, s=5)\n",
    "axes[1].plot([0, max(xgb_predictions)], [0, max(xgb_predictions)], 'r--')\n",
    "axes[1].set_xlabel('XGBoost Predictions ($)')\n",
    "axes[1].set_ylabel('Multimodal Predictions ($)')\n",
    "axes[1].set_title('XGBoost vs Multimodal Predictions')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/prediction_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save best multimodal model\n",
    "torch.save(results[best_fusion]['model'].state_dict(), '../outputs/multimodal_model.pth')\n",
    "\n",
    "# Save XGBoost model\n",
    "with open('../outputs/xgboost_model.pkl', 'wb') as f:\n",
    "    pickle.dump(tabular_model.model, f)\n",
    "\n",
    "# Save preprocessor\n",
    "preprocessor.save_preprocessor('../outputs/preprocessor.pkl')\n",
    "\n",
    "print(\"Models saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Results\n",
    "\n",
    "| Model | RMSE | R² Score |\n",
    "|-------|------|----------|\n",
    "| XGBoost Baseline | $129,486 | 0.8664 |\n",
    "| Multimodal Fusion | ~$125,000 | ~0.88 |\n",
    "| **EfficientNet + LightGBM + KNN** | **$111,857** | **0.9003** |\n",
    "\n",
    "### Key Findings\n",
    "1. Satellite imagery provides additional context about property surroundings\n",
    "2. KNN neighborhood features significantly improve predictions\n",
    "3. LightGBM with early stopping prevents overfitting\n",
    "4. The improved model achieves 13.6% RMSE reduction over baseline\n",
    "\n",
    "### Best Model Architecture\n",
    "- **Image Encoder**: EfficientNet-B0 (pretrained on ImageNet) → 256-dim embeddings\n",
    "- **KNN Features**: 15 nearest neighbors based on geographic coordinates\n",
    "- **Final Model**: LightGBM gradient boosting on combined features (294 total)\n",
    "\n",
    "### Next Steps\n",
    "- Run `python run_improved_pipeline.py` for the best results\n",
    "- Experiment with different CNN architectures\n",
    "- Incorporate additional data sources (street view, POI data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
